\documentclass[12pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage{graphicx,amssymb,hyperref}
\usepackage{wasysym,multirow,stmaryrd}
\usepackage{algorithm,algorithmic,color,tabularx}

\newcommand{\assignment}[4]{
\thispagestyle{plain} 
\newpage
\setcounter{page}{1}
\noindent
\begin{center}
\framebox{ \vbox{ \hbox to 6.28in
{\bf CS6140: Machine Learning \hfill Spring 2017}
\vspace{4mm}
\hbox to 6.28in
{\hspace{2.5in}\large\mbox{Problem Set #1}}
\vspace{4mm}
\hbox to 6.28in
{{\hfill {\em Official Due Date: #3}}}
\hbox to 6.28in
{{\it Handed Out: #2 \hfill Extended Due Date: #4}}
}}
\end{center}
}

\newcommand{\solution}[3]{
\thispagestyle{plain} 
\newpage
\setcounter{page}{1}
\noindent
\begin{center}
\framebox{ \vbox{ \hbox to 6.28in
{\bf CS6140: Machine Learning \hfill Spring 2017}
\vspace{4mm}
\hbox to 6.28in
{\hspace{2.5in}\large\mbox{Problem Set #1 Solutions}}
\vspace{4mm}
\hbox to 6.28in
{{\bf #2 \hfill Submitted: #3}}
}}
\end{center}
}

\newcommand{\red}[1]{
\textcolor{red}{#1}
}

\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}

\begin{document}

\assignment{3}{March 22, 2017}{April 4, 2017}{April 10, 2017}

\begin{footnotesize}
\begin{itemize}

\item
Please submit your solutions via your CCIS {\tt github} account.   

\item
Materials associated with this problem set are available at\\ \url{https://github.ccs.neu.edu/cs6140-03-spring2017/materials}.

\item
I encourage you to discuss the homework with other members of the class. The goal of the homework is for you to learn the course material. However, you should write your own solution.

\item
Please keep your solution brief, clear, and legible.  If you are feeling generous, I would {\em really} appreciate typed solutions (and if you plan on publishing CS/Math/Engineering research, this is actually a good exercise) -- see the source material if you would like to use \LaTeX{} to do this.

\item
I encourage you to ask questions before class, after class, via email, or the Piazza QA section. However, please do not start e-mailing me questions the night before the homework is due. $\smiley$

\end{itemize}
\end{footnotesize}

\begin{enumerate}
\item
{\bf [Classification via Linear Programming -- 15 points]}

Let $\mathcal{S}$ represent a training dataset of size $m$ where each $\mathbf{x}_i \in \mathbb{R}^d$ is a $d$-dimensional vector and $y_i \in \{-1, 1\}$ is the corresponding label.  Let $\mathbf{w} \in \mathbb{R}^d$ be the learned weight vector and $\theta$ be the threshold value such that we predict $h(\mathbf{x}_i) = 1$ if and only if $\mathbf{w}^T \mathbf{x}_i + \theta \geq 0$.  Else, $h(\mathbf{x}_i) = -1$.\footnote{Note that this is precisely the setting you are used to by this point.}

Instead of the SVM formulation, consider the following linear program formulation:\footnote{This is essentially a simplified variant of K.P. Bennett and O.L. Mangasarian, Robust Linear Programming Discrimination of Two Linearly Inseparable Sets. Optimization Methods and Software 1, 1992, 23-34. (\url{http://research.cs.wisc.edu/techreports/1991/TR1054.pdf})}

\begin{eqnarray}
\min & & z = \xi \\
\mbox{s.t.} & & y_i (\mathbf{w}^T \mathbf{x}_i + \theta) \geq 1 - \xi \hspace{0.2in} \forall (\mathbf{x}_i, y_i) \in \mathcal{S} \label{eqn:constraints} \\
& & \xi \geq 0
\end{eqnarray}

%\begin{enumerate}
%\item
%{\bf [15 points]}
%Prove that $\mathcal{S}$ is linearly separable if and only if it satisfies Equation~\ref{eqn:constraints} with $\xi = 0$.  Note that your argument will likely be very similar to the SVM formulation.\footnote{Hint: Assume that $\mathcal{S}$ is linearly separable. $\mathcal{S}$ is a finite set, so it has a positive example that is {\em closest} to the hyperplane among all positive examples. Similarly, there is a negative example that is {\em closest} to the hyperplane among negative examples. Consider their distances and use them to show that condition (\ref{eqn:constraints}) holds. Then show the other direction.}

%\item
%{\bf [10 points]}
A (restricted) general form of a linear program specification can be stated as
%
\begin{eqnarray}
\min & & z = \mathbf{c}^T \mathbf{x} \\
\mbox{s.t.} & & \mathbf{A} \mathbf{x} \geq \mathbf{b}
\end{eqnarray}
%
where $\mathbf{c}$ is often referred to as the {\em cost vector} and $z$ as the {\em objective function}.\footnote{This formulation also closely parallels the linear programming representation found in {\em Pattern Classification}, by Duda, Hart, and Stork.}

Please rewrite the linear program given by Equations (1-3) in terms of $\mathbf{c}$, $\mathbf{x}$, $\mathbf{A}$, and $\mathbf{b}$.  Note that given this formulation, you can use Matlab, Xpress, CVXOPT, GLPK, or many other LP solvers (including open-source) to solve this linear discriminant formulation.\footnote{Not that I am asking you to do this -- although it is useful to do at some point in your life...}

\item
{\bf [Online Learning -- 85 points]}

The purpose of this problem set is to compare the respective performance of a few well-known online learning algorithms by comparing their performance on a synthetic dataset. While the assignments thus far have placed an emphasis on using {\em real} data, simulations are often better suited for understanding specific properties of learning algorithms.

Specifically, we are going to explore learning a linear function resulting from training on data generated by an $l$-of-$m$-of-$n$ function.  An $l$-of-$m$-of-$n$ is defined over a $n$-dimensional Boolean vector where there is a defined subset of $m$ attributes such that $f(\mathbf{x}) = 1$ if and only if at least $l$ of these $m$ attributes are active in $\mathbf{x}$.  Note that this is a linear function,\footnote{If you don't see this immediately, spend some time convincing yourself.} meaning that \textsc{Winnow} and \textsc{Perceptron} are able to represent the target concept (i.e., the target concept is contained in the hypothesis space).

\subsection*{Algorithms}

Specifically, you must implement two online learning algorithms: \textsc{Perceptron} and \textsc{Winnow} (with and without margin in both cases) as defined below for Boolean vectors $\mathbf{x} \in \{0,1\}^n$.  Note that these are very closely related algorithms with the most significant different being the update rules.

\begin{algorithm}[h]
\begin{algorithmic}
   \STATE {\bfseries Input:} Training data $\mathcal{S}$, Number of rounds $T$, Learning rate $\eta$, Estimated margin $\gamma$
   \vspace{0.5em}
   \hrule
   \vspace{0.5em}
   \STATE $\mathbf{w} \leftarrow \mathbf{0}$; $b \leftarrow 0$; $t \leftarrow 0$
   \WHILE {$t < T$}
      \FORALL {$(\mathbf{x},y) \in \mathcal{S}$}
         \IF {$y (\mathbf{w} \cdot \mathbf{x} + b) \leq \gamma$} 
            \STATE $\mathbf{w} \leftarrow \mathbf{w} + \eta y \mathbf{x}$
            \STATE $b \leftarrow b + \eta y$
         \ENDIF
      \ENDFOR
      \STATE $t \leftarrow t + 1$
   \ENDWHILE
   \vspace{0.5em}
   \hrule
   \vspace{0.5em}
   \STATE {\bfseries Output:} Learned hypothesis $\mathbf{w}$ 
\caption{\textsc{Perceptron}}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\begin{algorithmic}
   \STATE {\bfseries Input:} Training data $\mathcal{S}$, Number of rounds $T$, Learning rate $\eta$,  Threshold $\theta$, Estimated margin $\gamma$
   \vspace{0.5em}
   \hrule
   \vspace{0.5em}
   \STATE $\mathbf{w} \leftarrow \mathbf{1}$; $t \leftarrow 0$
   \WHILE {$t < T$}
      \FORALL {$(\mathbf{x},y) \in \mathcal{S}$}
         \IF {$y (\mathbf{w} \cdot \mathbf{x} - \theta) \leq \gamma$}
%            \STATE $\mathbf{w} \leftarrow \mathbf{w} \circ \left(\eta^y \mathbf{x} + \mathbf{x}^{\perp} \right)$
            \STATE $\mathbf{w} \leftarrow \mathbf{w} \circ \eta^{y \mathbf{x}}$
         \ENDIF
      \ENDFOR
      \STATE $t \leftarrow t + 1$
   \ENDWHILE
   \vspace{0.5em}
   \hrule
   \vspace{0.5em}
   \STATE {\bfseries Output:} Learned hypothesis $\mathbf{w}$ 
\caption{\textsc{Winnow}}
\label{alg:winnow}
\end{algorithmic}
\end{algorithm}

\subsection*{Some Notes}
While it is difficult for me to preemptively guess which problems you will encounter, here are some notes that I have observed to be confusing for some.
%
\begin{itemize}
\item
For \textsc{Perceptron} without margin, if we set $[\mathbf{w} \; b] = \mathbf{0}$ (i.e., the concatenation of $\mathbf{w}$ and $b$), the learning rate has no effect.  Therefore, we will just set $\eta = 1$.
\item
For \textsc{Perceptron} with margin, the relationship between $\eta$ and $\gamma$ are closely related -- updates with a large $\eta$ will more likely make $y (\mathbf{w} \cdot \mathbf{x} + b) > \gamma$.  We will set $\gamma = 1$ and experiment with $\eta = \{1.5, 0.25, 0.03, 0.005, 0.001\}$.
\item
Since $l$-of-$m$-of-$n$ functions are monotone, we can use the simplest version of \textsc{Winnow} as shown, noting that we {\bf do not} update $\theta$.  The parameters we will consider includes $\eta = \{1.1, 1.01, 1.005, 1.0005, 1.0001\}$.
\item
\textsc{Winnow} with margin is somewhat sensitive.  Therefore, while we will experiment with varying $\eta = \{1.1, 1.01, 1.005, 1.0005, 1.0001\}$ and $\gamma = \{2.0, 0.3, 0.04, 0.006, 0.001\}$.
\end{itemize}

\clearpage
\subsection*{Generating Data}

Data is generated as follows.  First, the label is generated from a Bernoulli distribution with $\mu=0.5$.\footnote{Yes, you too can now make a fair coin flip seem sophisticated.}

\begin{itemize}
\item
For each positive example, select a number of active features from the interval $l \leq a \leq m$.  Secondly, select randomly from a uniform distribution $a$ attributes amongst $x_1, \ldots, x_m$ and set to 1.  Set the other $m - a$ attributes to 0.  Set the remainder of the $n-m$ attributes (e.g., $x_{m+1}, \ldots, x_{n}$) to 1 uniformly with a probability of 0.5.
\item
For each negative example, select a number of active features from the interval $0 \leq a \leq l-1$.  Secondly, select randomly from a uniform distribution $a$ attributes amongst $x_1, \ldots, x_m$ and set to 1.  Set the other $m - a$ attributes to 0.  Set the remainder of the $n-m$ attributes (e.g., $x_{m+1}, \ldots, x_{n}$) to 1 uniformly with a probability of 0.5.
\end{itemize}
%
\noindent
You have been provided with the Java file {\tt GenerateLMN.java} which can be used to generate this data in {\tt LIBSVM} format (sparse representation).  Note that we are manipulating $x_1, \ldots, x_m$ to represent the relevant portion of the function out of convenience (as there is no loss of generality).  However, you should {\em not} use this information to influence your learning algorithms.

\subsection*{Tuning Parameters}

While learning algorithms aren't generally as sensitive as it may seem in this assignment, one purpose of this assignment is to learn how to set hyper-parameters (e.g., $\eta, \gamma$, etc.).\footnote{Note that the process I describe also works for fixed-size data (although there are other reasonable protocols); for this specific problem, we could actually do this differently since we can generate an arbitrary amount of data.}  

To set the hyper-parameters, we will set aside two distinct subsamples of the training data, $\mathcal{D}_1$ and $\mathcal{D}_2$, each consisting of 10\% of the training data.  For each set of hyper-parameters settings, training the desired algorithm on $\mathcal{D}_1$ by running the algorithm twenty times over the data.  Based on the resulting learned parameters, evaluate this model on $\mathcal{D}_2$ and record the resulting accuracy.

Choosing the set of hyper-parameters that achieve the best performance on $\mathcal{D}_2$, conduct the experiments described in each section.  Note that you will be testing the outer product of hyper-parameters settings, meaning that two hyper-parameters with five values each will result in testing 25 sets of hyper-parameters. For your own sanity, I suggest writing code such that this can be automated.

\subsection*{Experiments}

\begin{enumerate}
\item
{\bf [20 points] Counting Mistakes}\\

The first set of experiments is to evaluate the specified learning algorithms with two target function configurations: (a) $l=10, m=100, n=500$ and (b) $l = 10, m=100, n=1000$.

In each case, your experiments should consist of the following steps:

\begin{enumerate}
\item
Generate a {\em clean}\footnote{meaning {\em not} noisy} 50,000 instance dataset for each specified $\{l,m,n\}$ configuration.
\item
Fill in the Table~\ref{table:exp1} with the best performing hyper-parameters:
%
\begin{table}[htb]
\begin{center}
\begin{tabular}{|l|c|c|C{0.6in}|c|c|C{0.6in}|}
\hline
Algorithm & \multicolumn{3}{|c|}{$n=500$} & \multicolumn{3}{|c|}{$n=1000$} \\
\hline
& Params & Acc($\mathcal{D}_2$) & M & Params & Acc($\mathcal{D}_2$) & M \\
\hline
\textsc{Perceptron}($\gamma = 0$) & & & & & & \\
\hline
\textsc{Perceptron}($\gamma > 0$) & & & & & & \\
\hline
\textsc{Winnow}($\gamma = 0$) & & & & & & \\
\hline
\textsc{Winnow}($\gamma > 0$) & & & & & & \\
\hline
\end{tabular}
\end{center}
\caption{Tuning Hyper-parameters for Experiment 1}
\label{table:exp1}
\end{table}
%
\item
For each of the four algorithms, run the algorithm over the generated dataset (of 50,000 instances) once and keep track of the number of mistakes, $M$, that each algorithm makes.  Note that a mistake is different than an update (as the margin \textsc{Perceptron}/\textsc{Winnow} updates for many cases where no mistake is made).
\item
Plot the cumulative number of mistakes $M$ versus the number of instances $(0 \leq N \leq 50,000)$ observed.  For each configuration, plot all four curves on the same graph such that they can be easily compared.  In each case, the $x$-axis should be the number of instances observed, $N$, and the $y$-axis should be the number of mistakes made $M$. You should have two graphs (one for each dataset) with four curves on each.  Please clearly label your graphs.\footnote{If you are having memory issues, you can consider plotting the cumulative error every 10 or 100 instances.}
\end{enumerate}
%
\item
{\bf [30 points] Learning Curves}\\

The second set of experiments is to construct learning curves.  We will begin by setting $l=10, m=20$ and vary $n$ such that $40 \leq n \leq 200$ in increments of 40.  For each of the 5 different functions, begin by generating 50,000 {\em clean} instances with the specified $\{l,m,n\}$ configuration.  

As in the previous section, fill in Table~\ref{table:exp2}.
%
\begin{table}[htb]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{2}{|c|}{Algorithm} & \textsc{Perceptron} & \textsc{Perceptron}($\gamma$) & \textsc{Winnow} & \textsc{Winnow}($\gamma$) \\
\hline
\multirow{3}{*}{$n=40$} & Params & & & & \\
\cline{2-6}
& Acc($\mathcal{D}_2$) & & & & \\
\cline{2-6}
& M & & & & \\
\hline
\multirow{3}{*}{$n=80$} & Params & & & & \\
\cline{2-6}
& Acc($\mathcal{D}_2$) & & & & \\
\cline{2-6}
& M & & & & \\
\hline
\multirow{3}{*}{$n=120$} & Params & & & & \\
\cline{2-6}
& Acc($\mathcal{D}_2$) & & & & \\
\cline{2-6}
& M & & & & \\
\hline
\multirow{3}{*}{$n=160$} & Params & & & & \\
\cline{2-6}
& Acc($\mathcal{D}_2$) & & & & \\
\cline{2-6}
& M & & & & \\
\hline
\multirow{3}{*}{$n=200$} & Params & & & & \\
\cline{2-6}
& Acc($\mathcal{D}_2$) & & & & \\
\cline{2-6}
& M & & & & \\
\hline
\end{tabular}
\end{center}
\caption{Tuning Hyper-parameters for Experiment 2}
\label{table:exp2}
\end{table}
%
To run the experiment,

\begin{enumerate}
\item
Present an example to the learning algorithm.
\item
Again, keep track of the number of mistakes, $M$, the algorithm makes.
\end{enumerate}

The way we will measure convergence is that we will let the algorithm run until $S$ consecutive examples are presented such that no mistakes are made (i.e, the current hypothesis {\em survives} for 1000 instances).  Note that we are able to do this since we know that the algorithms can learn the target function.  Once $S$ instances are encountered, record $M$ at this point and halt.  For each algorithm, plot a curve of $M$ (on the $y$-axis) as a function of $n$ (on the $x$-axis) such that there are four curves, each determined by the five points associated with each value of $n$, on a single plot.

Note that you may have to play a bit with the value of $S$.  I would use $S=1000$ as a good starting point.  However, if this is too large such that the algorithm does not halt before $N=50,000$, you may have to lower this value.  Note that you must use the same value of $S$ for all experiments for valid  comparisons.

%
\item
{\bf [35 points] Batch Performance}\\

In this case, we will run the online algorithms in batch mode to gain understanding of the relative performance in more common scenarios.  These experiments should be conducted as follows:

\begin{enumerate}

\item
For a given $\{l,m,n\}$ configuration, generate a {\em noisy} 50,000 instance training set and {\em clean} 10,000 instance testing set.  We will flip each label with probability 0.05 and each attribute with probability 0.001 using {\tt GenerateLMN.java}.
\item
Optimize the hyper-parameters as in previous sections.  Note that technically, since we are running online algorithms in batch mode, $T$ is another hyper-parameter -- however, we will just set $T=20$ as you have probably had enough by now (although if you find something interesting, I would be most receptive).
\item
Using these hyper-parameters, train the model with the 50,000 training examples and evaluate your model on the testing data.  Report the accuracy of each respective learning algorithm.
\end{enumerate}

Repeat this experiment with the following three $\{l,m,n\}$ configurations.

\begin{itemize}
\item
$l=10, m=100, n=1000$
\item
$l=10, m=500, n=1000$
\item
$l=10, m=1000, n=1000$
\end{itemize}

For each $\{l,m,n\}$ configuration, the same training and testing data should be used for all appropriate comparisons.  This can either be accomplished by using files or setting the random seed appropriately.  You should generate results similar to Table~\ref{table:exp3}.
%
\begin{table}[htb]
\footnotesize
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
Algorithm & \multicolumn{3}{|c|}{$m=100$} & \multicolumn{3}{|c|}{$m=500$} & \multicolumn{3}{|c|}{$m=1000$} \\
\hline
& Params & Acc($\mathcal{D}_2$) & Acc(Test) & Params & Acc($\mathcal{D}_2$) & Acc(Test) & Params & Acc($\mathcal{D}_2$) & Acc(Test) \\
\hline
\textsc{Perc.} & & & & & & & & & \\
\hline
\textsc{Perc.}($\gamma$) & & & & & & & & & \\
\hline
\textsc{Winnow} & & & & & & & & & \\
\hline
\textsc{Winnow}($\gamma$) & & & & & & & & & \\
\hline
\end{tabular}
\end{center}
\caption{Tuning Hyper-parameters for Experiment 3}
\label{table:exp3}
\end{table}

\end{enumerate}

\subsection*{What to submit}

\begin{itemize}
\item
A detailed, yet concise report.  In addition to the requested information, summarize the findings.  Discuss differences in performance of the algorithms and attempt to explain why.  Were the results consistent across all experiments?  If possible, try to make the report interesting.  Note that these experiments were (heavily) influenced by [Kivinen, Warmuth, and Auer; The Perceptron Algorithm versus Winnow: Linear versus Logarithmic Mistake Bounds When Few Input Variables are Relevant, Artificial Intelligence, pg 325-343, 1997] if you would like to do some reading.
\item
Two plots from the first experiment and one plot from the second experiment.  Please clearly label these.
\item
One table for the third experiment.
\item
Tables associated with each experiment.
\item
Source code.  Submit all relevant files via {\tt github}.  You are free to use the programming language of your choice.  However, please include a {\tt README} file that provides instructions on compiling and running your program.  Of course, a shell script would be greatly appreciated.

\item
Use your CCIS github repository to submit all relevant files.  You are free to use the programming language of your choice, but please attempt to conform to the instructions above.  To be safe, try submitting something {\bf before} the assignment deadline.

\item
The code you submit must be your own. If you find/use information about specific algorithms from the Web, etc., be sure to cite the source(s) clearly in your source code. 
\end{itemize}

\end{enumerate}

\section*{Appendix: Linear Programming}

In this appendix, we will walk through a simple linear programming example.\footnote{Note that SVM uses {\em quadratic} programming (QP) which means that the objective function includes a quadratic term.}  If you want to read more on the topic, a good reference is {\em Linear Programming: Foundations and Extensions} by Vanderbei.  Some classic texts include {\em Linear Programming} by Chvatal; and {\em Combinatorial Optimization: Algorithms and Complexity} by Papadimitrou and Steiglitz (in particular, the beginning of chapter $2$ may be helpful).  A widely available (albeit incomplete) reference is {\em Introduction to Algorithms} by Cormen, Leiserson, Rivest, and Stein.

\vspace{3mm}

\textbf{Example: }
Consider the following problem.\footnote{The problem closely resembles an instantiation of the {\em diet problem} by G.~J.~Stigler, {\em The Cost of Subsistence}, 1945.} You are given a choice of three foods, namely eggs at a cost of $\$0.10$ an egg, pasta at a cost of $\$0.05$ a bowl, and yogurt at a cost of $\$0.25$ a cup.  An egg $(t_1)$ provides $3$ portions of protein, $1$ portion of carbohydrates, and $2$ portions of fat.  A bowl of pasta $(t_2)$ provides $1$ portion of protein, $3$ portions of carbohydrates, and no portions of fat.  A cup of yogurt $(t_3)$ provides $2$ portions of protein, $2$ portions of carbohydrates, and $1$ portion of fat.  You are required to consume at least $7$ portions of protein and $9$ portions of carbohydrates per day and are not allowed to consume more than $4$ portions of fat.  In addition, you obviously may not consume a negative amount of any food.  The objective now is to find the cheapest combination of foods that still meet your daily nutritional requirements. 

\clearpage
This can be written as the following linear program:

\begin{equation}
z = 0.1t_1 + 0.05t_2 + 0.25t_3 \to \min
\end{equation}
\begin{equation}
3t_1 + t_2 + 2t_3 \geq 7 \\
\end{equation}
\begin{equation}
t_1 + 3t_2 + 2t_3 \geq 9 \\
\end{equation}
\begin{equation}
-2t_1 - t_3 \geq -4 \\
\label{constraint3}
\end{equation}
\begin{equation}
t_i \geq 0 \hspace{0.3in} \forall i
\end{equation}

Note that inequality (\ref{constraint3}) of the LP is equivalent to $2t_1 + t_3 < 4$. This corresponds to:

\begin{tabular}{cccc}
${\mathbf A} = \left ( \begin{array}{ccc}3 & 1 & 2\\1 & 3 & 2\\-2 & 0 & -1 \end{array} \right ) $
&
$\vec{b} = \left ( \begin{array}{c}7\\9\\-4\end{array} \right ) $
&
$\vec{c} = \left ( \begin{array}{c}0.1\\0.05\\0.25\end{array} \right ) $
&
$\vec{t} = \left ( \begin{array}{c}t_1\\t_2\\t_3\end{array} \right ) $
\end{tabular}

\vspace{3mm} 

To solve this program using Matlab:
\begin{verbatim}
     c = [0.1; 0.05; 0.25];
     A = [3 1 2; 1 3 2; -2 0 -1];
     b = [7; 9; -4];
     lowerBound = zeros(3, 1); % this constrains t >= 0
     [t, z] = linprog(c, -A, -b, [], [], lowerBound)
\end{verbatim}

The results of this linear program show that to meet your nutritional requirements at a minimum cost, you should eat $1.5$ eggs, $2.5$ bowls of pasta, and no cups of yogurt, and the cost for such a diet is $\$0.275$.\footnote{Note that this is not intended to be actual nutritional advice.  Your mileage may vary.}

\end{document}